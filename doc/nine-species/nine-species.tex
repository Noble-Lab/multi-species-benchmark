\documentclass{article}
\usepackage{graphicx} % Allow insertion of graphics.
\usepackage{authblk} % Better layout of affiliations.
\usepackage[margin=1in]{geometry} % Set margins.
\usepackage{xcolor} % Allow colored text.
\usepackage{url} % Allow URLs.
\usepackage[sort&compress]{natbib} % Better bibliography formatting.

\newcommand{\fixme}[1]{{\color{red}{\bf FIXME: #1}\color{black}}}

\title{A multi-species benchmark for training and validating large scale mass spectrometry proteomics machine learning models}

\author[1,2]{William Stafford Noble}

\affil[1]{Department of Genome Sciences, University of Washington}
\affil[2]{Paul G.\ Allen School of Computer Science and Engineering, University of Washington}

\date{}

\begin{document}

\maketitle

\fixme{Instructions at
  \url{https://www.nature.com/sdata/publish/submission-guidelines}.}

\begin{abstract}
  \fixme{Insert abstract here.}
\end{abstract}

\section*{Background \& Summary}

\textit{De novo} sequencing of proteomics tandem mass spectrometry data, in which observed fragmentation spectra are translated into corresponding peptide sequences, has been an open challenge for more than 40 years \cite{sakurai1984paas}.
Recently, as in many other areas of science, considerable progress toward solving this challenge has been made using deep learning, in which multi-layer neural networks with millions of parameters are trained to generate peptide sequences from observed spectra.
The first such deep learning method, DeepNovo \cite{tran2017denovo}, has been followed by at least 22 additional publications (reviewed in \cite{XXX}).

The standard method for evaluating these \textit{de novo} sequencing methods is to use a gold standard produced via database search.
In this approach, mass spectrometry data derived from a single species is searched against the reference proteome for that species, yielding a ranked list of peptide-spectrum matches (PSMs).
Including in the peptide database a collection of reversed or shuffled ``decoy'' peptides provides a rigorous way to set a threshold in this list of PSMs while controlling the false discovery rate (FDR) among the PSMs above the threshold \cite{elias:target}.
The resulting set of high-confidence PSMs can be used either to train or evaluate a \textit{de novo} sequencing model.

Some version of the above protocol has been used to develop labeled training and validation data for essentially every published deep learning \textit{de novo} sequencing method.
One exception is cases such as \fixme{XXX} that use spectra from synthesized peptide sequences for training.
However, even in these cases, a gold standard derived from database search is used for evaluation of the method.

Unfortunately, creating a high quality gold standard set of labeled spectra can be tricky.
One challenge is ensuring that the search strategy employs appropriate parameters.
For instance, one widely used benchmark dataset \cite{tran2017denovo} used a search strategy that failed to account for missassigned isotopic peaks during the acquisition stage.
This error led to frequently assigning a deamidation modification, when the observed mass shift was better explained by an isotopic mass shift on the precursor \textit{m/z} \cite{bittremieux:deep}.
A second challenge relates to the notion of train/test ``leakage,'' in which information used to train the model leaks into the evaluation procedure.
In the \textit{de novo} setting, a common mistake is to randomly segregate a given set of labeled spectra into training and test sets, without regard to the associated peptides.
As a result, spectra generated by the same peptide sequence may occur in both the training and test sets.
Such duplicated peptides give an unfair advantage to the sequencing method, and the leakage will be even more useful to parameter-rich methods that are capable of memorizing many features of the training data.

In this work, we revisit the nine-species benchmark dataset that was employed in the first deep learning \textit{de novo} sequencing method, DeepNovo \cite{trans2017denovo}.
This is a widely used dataset, which has been employed for training or evaluation in at least \fixme{XXX} subsequent studies \cite{XXX}.
The setup is quite straightforward.
The authors downloaded nine publicly available datasets, all of which were generated on a Thermo Scientific Q Exactive mass spectrometer, and each of which was carried out in a different species.
Each dataset was searched against the corresponding reference proteome, using a target-decoy strategy to accept a set of PSMs subject to a PSM-level FDR threshold of 1\%.
Because the data are derived from different species, the peptides in each set are largely (but not entirely) disjoint.
To use the benchmark, it is typical to apply a cross-validation strategy, in which a model is trained on eight species and tested on the held-out species, and the procedure is repeated nine different ways.

In developing our Casanovo \textit{de novo} sequencing model, we identified several problems with the nine-species benchmark \cite{yilmaz2023sequence}.
These included the deamidation problem mentioned above, as well as some uncertainty regarding how the FDR was controlled.
Perhaps most importantly, we recognized that a non-negligible proportion of peptides are shared among the different species, with the highest overlap between human and mouse.

In light of these difficulties, we downloaded the same datasets from the PRIDE repository and systematically reanalyzed all of the data, using a standard search procedure---the Tide search engine \cite{diament2011faster} followed by Percolator \cite{kall2007semi-supervised} with PSM-level FDR control at 1\%.
We then filtered the PSMs to prevent any peptide sequence from appearing in more than one species.
Finally, because some of the single-species datasets are markedly larger than others, we produced a more balanced version of the dataset.
Hence, we make publicly available all three versions of this dataset: the original, unfiltered dataset if you simply want to train a model with as much data as possible (``overlap''), the peptide-disjoint dataset that can be used to avoid train/test leakage (``main''), and the reduced peptide-disjoint dataset if you want your analysis to run more quickly (``balanced'').
In addition, we make available all of the intermediate files, for use in validating the benchmark.

\section*{Methods}

\subsection*{Data sets}

\begin{table}
\scriptsize
\centering
\begin{tabular}{clcrrrr}
\hline
PRIDE & Species & Uniprot & Files & Spectra & precursor & fragment \\
\hline
PXD005025 & \textit{Vigna mungo} & UP000087766 & 24 & 932848 & 20 & 0.05 \\
PXD004948 & \textit{Mus musculus} & UP000000589 & 13 & 306786 10 & 0.05 \\
PXD004325 & \textit{Methanosarcina mazei} & UP000033058  & 72 & 3728183 & 10 & 0.05 \\
PXD004565 & \textit{Bacillus subtilis} & UP000001570 & 106 & 4336428 & 30 & 0.05 \\
PXD004536 & \textit{Candidatus endoloripes} & UP000094849 & 11 & 2272023 & 20 & 0.05 \\
PXD004947 & \textit{Solanum lycopersicum} & UP000004994 & 60 & 603506 & 15 & 0.05 \\
PXD003868 & \textit{Saccharomyces-cerevisiae} & UP000002311 & 27 & 1477397 & 20 & 0.05 \\
PXD004467 & \textit{Apis mellifera} & UP000005203 & 17 & 823169 & 20 & 0.05 \\
PXD004424 & \textit{H. sapiens} & UP000005640 & 26 & 684821 & 20 & 0.02 \\
\hline
Total & & & 343 & 15,165,161 \\
\end{tabular}
\caption{{\bf The nine-species benchmark.}
The final two columns specify the precursor window size (in ppm) and fragment bin size (in Da) used in the database search step.
No reference proteome is available for \textit{Vigna mungo}, so the proteome for the closely related species \textit{Vigna radiata} was used instead.
}
\label{tab:benchmark}
\end{table}

For our benchmark, we used the same nine studies originally identified by Tran {\em et al.} \cite{tran2017denovo}
\begin{enumerate}
\item Paiva \textit{et al.}\ investigated the protein expression response of the cowpea plant (\textit{Vigna unguiculata}) to infection by \textit{Cowpsea severe mosaic virus} (CSMV) by carrying out label-free proteomic analysis of cowpea leaves that were inoculated with CSMV compared to mock inoculation controls \cite{paiva2016label}.
\item Nevo \textit{et al.}\ studied a rare autosomal recessive lysosomal storage disorder, cystinosis, by carrying out SILAC proteomic analysis of engineered mouse cell lines that harbor a known pathenogenic mutation of the causative gene, \textit{CTNS} \cite{nevo2017impact}.
\item Cassidy \textit{et al.}\ evaluated two different analytical approaches for carrying out full proteome analysis while identifying short open reading frames: high/low pH reversed phase LC-MS bottom-up approach and a semi-top-down strategy involving separation of proteins a GelFree system followed by digestion and LC-MS analysis \cite{cassidy2016combination}. The experiments were carried out using the methane producing archaeon \textit{Methanosarcina mazei}.
\item Reuss \textit{et al.}\ carried out proteomic analyses on a series of minimized strains of the model bacterium, \textit{Bacillus subtilis}, with genomes reduced by $\sim$36\% \cite{reuss2017large}.
\item Petersen \textit{et al.}\ performed proteomic analysis of \textit{Candidatus endoloripes}, which are bacterial symbionts of the \textit{Lucinidae} family of marine bivalves \cite{petersen2016chemosynthetic}.
\item Mata \textit{et al.}\ characterized the proteome of the tomato pericarp at its ripe red stage \cite{mata2017depth}.
\item Seidel \textit{et al.}\ analyzed the global proteomic stress response in wildtype and two yeast knockout strains for the gene PBP1  \cite{seidel2017quantitative}.
\item Hu \textit{et al.}\ studied honeybees that exhibit a suite of behaviors (\textit{Varroa} sensitive hygiene---VSH) associated with infection with the \textit{Varroa destructor} virus \cite{hu2016proteome}.  Proteomic analysis was carried out on mushroom bodies and antennae of adult honeybees with and without VSH.
\item Cypryck \textit{et al.} characterized extracellular vesicles released from human primary macrophages after infection with influenza A viruses \cite{cypryk2017proteomic}.
\end{enumerate}
All nine studies were performed using a Thermo Scientific Q Exactive mass spectrometer.

% The PXD entry says the species is "Vigna mungo (Rice bean) (Black gram); NCBI TaxID: 3915;" but the paper talks about Vinga unguiculata.

We downloaded the RAW files from the corresponding PRIDE projects (Table~\ref{tab:benchmark}) and converted them to MGF format using the ThermoRawFileParser v1.3.4.
We downloaded the corresponding nine Uniprot reference proteomes and constructed a Tide index for each one, using Crux version 4.2.
Note that, for one species (\textit{Vigna mungo}) no reference proteome is available, so we used the proteome of the closely related species \textit{Vigna radiata}.

\begin{table}
\scriptsize
\centering
\begin{tabular}{lrrrrrr}
\hline
Species & \multicolumn{2}{c}{Overlap} & \multicolumn{2}{c}{Main} & \multicolumn{2}{c}{Balanced} \\
& PSMs & Peptides  & PSMs & Peptides  & PSMs & Peptides  \\
\hline
\textit{Vigna mungo} & & & 108514 & 12001 \\
\textit{Mus musculus} & & & 25541 & 5899 \\
\textit{Methanosarcina mazei} & & & 267333 & 15925 \\
\textit{Bacillus subtilis} & & & 1358337 & 30786 \\
\textit{Candidatus endoloripes} & & & 82290 & 8392 \\
\textit{Solanum lycopersicum} & & & 178413 & 49745 \\
\textit{Saccharomyces-cerevisiae} & & & 585593 & 19720 \\
\textit{Apis mellifera} & & & 194281 & 21559 \\
\textit{H. sapiens} & & & 44604 & 11289 \\
\hline
Total & & & 2,844,906 & 175,316 \\
\end{tabular}
\caption{{\bf Three versions of the nine-species benchmark.}
  The ``overlap'' dataset contains peptides that are associated with more than one species.
  The ``main'' dataset \fixme{XXX}
}
\label{tab:benchmark}
\end{table}

\subsection*{Database search and FDR control}

We assigned peptide labels to spectra using the Tide search engine followed by post-processing with Percolator.
In creating the Tide index, we specified specified Cys carbamidomethylation as a static modification and allowed for the following variable modifications: Met oxidation, Asn deamidation, Gln deamidation, N-term acetylation, N-term carbamylation, N-term NH$_{3}$ loss, and the combination of N-term carbamylation and NH$_{3}$ loss by using the tide-index options {\tt --mods-spec 1M+15.994915,\allowbreak 1N+0.984016,\allowbreak 1Q+0.984016 --nterm-peptide-mods-spec 1X+42.010565,\allowbreak 1X+43.005814,\allowbreak 1X-17.026549,\allowbreak 1X+25.980265 --max-mods 3}.
Note that one of the nine experiments (\textit{Mus musculus}) was performed using SILAC labeling, but we searched without SILAC modifications and hence include in the benchmark only PSMs from unlabeled peptides.
Tide automatically added to each index a shuffled decoy peptide corresponding to each target peptide.
Thereafter, each MGF file was searched against the corresponding index using the precursor window size and fragment bin tolerance specified in the original study (Table~\ref{tab:benchmark}).
The search engine employed XCorr scoring with Tailor calibration \cite{sulimov2020tailor}, and we allowed for 1 isotope error in the selection of candidate peptides.
All search results were then analyzed jointly per species using the Crux implementation of Percolator, with default parameters.
For the benchmark, we retained all PSMs with Percolator q value $<$ 0.01.
We identified 13 MGF files with fewer than 100 accepted PSMs, and we eliminated all of these PSMs from the benchmark.
The resulting benchmark (``overlap'') contains \fixme{XXX} million PSMs drawn from 343 RAW files.

\subsection*{Avoiding train/test leakage}

To avoid train/test leakage, we post-processed the PSMs to eliminate peptides that are shared between species.
Among the 229,984 unique peptides, we identified 3797 (1.7\%) that occur in more than one species.
For each such peptide, we selected one of the associated species at random and then eliminated all PSMs containing that peptide in other species.
Note that when identifying shared peptides between species, we considered all modified forms of a given peptide sequence to be the same.
Hence, if a given peptide appears in more than one species, then that peptide, including all its modified forms, is randomly assigned to a single species and eliminated from the others.
The final, non-redundant benchmark dataset (``main'') consists of 2.8 million PSMs corresponding to \fixme{XXX} peptides.

\subsection*{Balancing the benchmark}

At this stage, the benchmark was quite imbalanced, in the sense that some species had a much larger number of associated PSMs.
We therefore used a random downsampling procedure to produce a benchmark that is more evenly balanced across species.
Among the nine species, the one with the fewest PSMs is \textit{Mus musculus}, with 25,541.
Downsampling all of the other eight species to have 25,000 PSMs would reduce the size of the dataset from 2.8 million PSMs to 225,000---a reduction of 92\%.
To avoid producing such a small dataset, we therefore opted to downsample each dataset to approximately 100,000 PSMs.
This approach yields a slight imbalance, because three species have fewer than 100,000 PSMs (44,604 for \textit{H.\ sapiens} and 82,290 for \textit{Candidatus endoloripes}), while retaining a larger percentage of the original data.
Our downsampling procedure involved randomly permuting the order of the MGF files for each species and then selecting the files in order until at least 100,000 PSMs have been accepted.
The final, balanced benchmark dataset consists of \fixme{XXX} PSMs from \fixme{XXX} peptides.

\section*{Data Records}

The dataset contains files resulting from various stages of the benchmark:
\begin{itemize}
\item Spectrum files in MGF format, produced by msconvert.
\item Reference proteome files in FASTA format, downloaded from Uniprot.
\item Search results files for both targets and decoys, in tab-delimited format, produced by Tide.
\item PSM-level Percolator results files for targets, in tab-delimited format.
\item Annotated MGF files for all three versions of the benchmark (overlap, main, and balanced).
\end{itemize}
Also included are log files for the steps of the analysis pipeline carried out using Crux \cite{park2008rapid} (Tide indexing, Tide search, and Percolator).

\section*{Technical Validation}

\fixme{False discovery rate curves}

\section*{Code Availability}

\bibliographystyle{unsrt}
\bibliography{refs} % See https://github.com/Noble-Lab/noble-lab-references

\paragraph{Author Contributions}

\paragraph{Competing Interests}
These authors declare that they have no competing interests.
  
\end{document}
