\documentclass{article}
\usepackage{graphicx} % Allow insertion of graphics.
\usepackage{authblk} % Better layout of affiliations.
\usepackage[margin=1in]{geometry} % Set margins.
\usepackage{xcolor} % Allow colored text.
\usepackage{pifont} % Allow dingbats.
\usepackage[sort&compress]{natbib}
\usepackage{url}

%\usepackage[backend=biber, bibencoding=inputenc, style=chem-acs, articletitle=true, sorting=none, hyperref=auto, isbn=false, doi=true, maxbibnames=100, minbibnames=100, url=false, giveninits=true, autocite=superscript]{biblatex} % Using Biber for the bibliography.

% Make captions have small font.
\usepackage[font=scriptsize]{caption}

\newcommand{\fixme}[1]{{\color{red}{\bf #1}\color{black}}}

\title{Wrangling a de novo sequencing benchmark}

\author[1,2]{William Stafford Noble}

\affil[1]{Department of Genome Sciences, University of Washington}
\affil[2]{Paul G.\ Allen School of Computer Science and Engineering, University of Washington}

\date{}

\begin{document}

\maketitle

In any machine learning study, high quality data for training and validating the model is critical.
This paper \cite{wen2024multi-species} describes the result of an iterative process of data wrangling and quality control, which ultimately produced a benchmark dataset for de novo peptide sequencing from mass spectrometry data.

The behind-the-paper story here is about just how iterative this process can be.
In a sense, the first iteration of the benchmark is the version described in the initial DeepNovo paper \cite{tran2017denovo}, though for all I know that version itself represents multiple prior iterations.
We used that benchmark in the first paper describing our Casanovo de novo sequencing model \cite{yilmaz2022denovo}.
When we began working on the second paper about Casanovo \cite{yilmaz2024sequence}, we were motivated to create a new version of the benchmark because we weren't sure how the database searching and FDR control were done.

The process, as outlined in my lab notebook, ended up producing nine versions of the benchmark.
\begin{enumerate}
\item  I started by manually downloaded the nine reference proteome fasta files from Uniprot.
  To produce the first version of the benchmark, I wrote scripts to automatically download all the mass spectrometry data using ppx \cite{fondrie2021ppx}, convert to MGF format using ThermoRawFileParser, and search the data using the Tide search engine \cite{park2008rapid} followed by Percolator \cite{kall2007semi-supervised}.
  The peptide sequences for PSMs accepted at 1\% PSM-level FDR were then inserted into the MGF files, discarding any spectra that failed to be identified.
\item Initially, I used the same modifications that had been in the DeepNovo analysis.  However, we realized that we should make this benchmark consistent with the set of modifications in MassIVE-KnowledgeBase \cite{wang2018assembling}, because those were the modifications we were training Casanovo with.
\item For each of the nine species, I had initially used estimates of precursor \textit{m/z} tolerance and fragment bin size generated by our tool, Param-Medic \cite{may2017param-medic}.
  However, it seemed more defensible to use the parameters listed in the publications describing these datasets, so I switched to using those.
\item I discovered that in the initial download of the raw files, some of the downloads failed.
  So I fixed that problem and re-generated the benchmark.
\item I had initially created a version of the benchmark with one big MGF file per species, but this was problematic because scan numbers ended up being repeated.
  So I switched to creating one MGF file per raw file.
\item We found that some of the annotations in the original DeepNovo benchmark did not properly account for isotope errors (see Figure~5 in \cite{bittremieux2024deep}), so I added handling of isotope errors to my search parameters.
  Overall, this change did not make a big difference in the number of accepted PSMs.
  For one species (\textit{Vigna mungo}) the number dropped slightly; for all the others it increased by a few thousand PSMs. 
\item We realized that some peptides in the benchmark were shared between species, so I added a post-processing step to eliminate these shared peptides.
\item A Crux user pointed out that our benchmark did not actually contain any N-terminal modifications.
  It turns out that this was a known bug in the Tide search engine, which had been recently fixed.
  I therefore re-ran the entire search procedure to generate a new version of the benchmark.
\item One of the reviewers of the second Casanovo paper asked us to be sure that different modified forms of the same peptide sequence all be associated with a single species.
  This seemed like a good idea, so I made this change.
\item Unfortunately, Tide and Casanovo do not agree on how to represent a peptide containing modifications: Tide puts them in square brackets, whereas Casnovo leaves off the brackets but precedes the mass with a ``+''.
  I therefore added a cleaning step to convert all the Tide peptides to Casanovo format.
\end{enumerate}

As this list makes clear, the process of creating the revised nine-species benchmark and ensuring its quality has been iterative.
I fully expect to need to make additional updates to the benchmark as others begin using it, so I will continue to update our GitHub repository of scripts (\url{https://github.com/Noble-Lab/multi-species-benchmark}) and the benchmark itself (\url{https://zenodo.org/records/12819174}) as needed.

\bibliographystyle{unsrt}
\bibliography{refs} % See https://github.com/Noble-Lab/noble-lab-references

\end{document}
